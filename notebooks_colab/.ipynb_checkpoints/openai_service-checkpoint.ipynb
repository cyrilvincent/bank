{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 634,
     "status": "ok",
     "timestamp": 1705078016941,
     "user": {
      "displayName": "Grégory Deschamps",
      "userId": "00349060252068867806"
     },
     "user_tz": -60
    },
    "id": "T154f6qLU-FC",
    "outputId": "a8b11432-9dfb-4189-a569-ee3d54e1a6f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.12\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 696,
     "status": "ok",
     "timestamp": 1705078144964,
     "user": {
      "displayName": "Grégory Deschamps",
      "userId": "00349060252068867806"
     },
     "user_tz": -60
    },
    "id": "r3ea-cLgWL4e"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 327
    },
    "executionInfo": {
     "elapsed": 953,
     "status": "error",
     "timestamp": 1705078152643,
     "user": {
      "displayName": "Grégory Deschamps",
      "userId": "00349060252068867806"
     },
     "user_tz": -60
    },
    "id": "fJw7X2xIWaqc",
    "outputId": "cdde0476-bfa5-47da-b164-ced77fafb3eb"
   },
   "outputs": [
    {
     "ename": "SecretNotFoundError",
     "evalue": "Secret openai does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSecretNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-ad86497c153d>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'openai'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Colab Secret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/userdata.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m     46\u001b[0m     )\n\u001b[1;32m     47\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exists'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mSecretNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'access'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mNotebookAccessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSecretNotFoundError\u001b[0m: Secret openai does not exist."
     ]
    }
   ],
   "source": [
    "with open(\"data/openai/openai.env\", \"rb\") as f:\n",
    "    key = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 254
    },
    "executionInfo": {
     "elapsed": 231,
     "status": "error",
     "timestamp": 1705078175775,
     "user": {
      "displayName": "Grégory Deschamps",
      "userId": "00349060252068867806"
     },
     "user_tz": -60
    },
    "id": "wsD4yQ1vWeI1",
    "outputId": "d8271605-ef9c-4c2a-be64-5e478550009b"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'key' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-2facf14c184e>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# print(text)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m completion = client.chat.completions.create(model=\"gpt-3.5-turbo\",\n",
      "\u001b[0;31mNameError\u001b[0m: name 'key' is not defined"
     ]
    }
   ],
   "source": [
    "with open(\"data/openai/quelle_est_bleue.txt\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# print(text)\n",
    "\n",
    "client = OpenAI(api_key=key)\n",
    "\n",
    "completion = client.chat.completions.create(model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"De quoi parle ce texte? dans un contexte de plongeur pro\"},\n",
    "        {\"role\": \"user\", \"content\": text}\n",
    "    ]\n",
    ")\n",
    "\n",
    "res = completion.choices[0].message\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19042,
     "status": "ok",
     "timestamp": 1704739227508,
     "user": {
      "displayName": "Cyril Vincent",
      "userId": "11684947983570556386"
     },
     "user_tz": -60
    },
    "id": "83aVzyVEXQOM",
    "outputId": "13747086-6322-46a1-9193-b693ff2505ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import pandas as pd\n",
      "import sweetviz\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "pd.options.display.max_columns = None\n",
      "\n",
      "df = pd.read_csv('data/churn/churn.csv', delimiter=',')\n",
      "print(df.shape)\n",
      "\n",
      "# Anonymisation\n",
      "df = df.drop([\"RowNumber\", \"CustomerId\", \"Surname\"], axis = 1)\n",
      "\n",
      "# France only\n",
      "df = df[df.Geography == \"France\"]\n",
      "df = df.drop([\"Geography\"], axis = 1)\n",
      "print(df.describe().T)\n",
      "\n",
      "# my_report = sweetviz.analyze(df)\n",
      "# my_report.show_html()\n",
      "\n",
      "# Discretisation?\n",
      "print(df.nunique())\n",
      "\n",
      "# Etude pragmatique sur valeurs discretes (categorielles)\n",
      "fig, axarr = plt.subplots(2, 2)\n",
      "sns.countplot(x='Gender', hue = 'Exited',data = df, ax=axarr[0][0])\n",
      "sns.countplot(x='HasCrCard', hue = 'Exited',data = df, ax=axarr[0][1])\n",
      "sns.countplot(x='IsActiveMember', hue = 'Exited',data = df, ax=axarr[1][0])\n",
      "plt.show()\n",
      "\n",
      "# Etude pragmatique sur valeurs continues\n",
      "fig, axarr = plt.subplots(3, 2)\n",
      "sns.boxplot(y='CreditScore',x = 'Exited', hue = 'Exited',data = df, ax=axarr[0][0])\n",
      "sns.boxplot(y='Age',x = 'Exited', hue = 'Exited',data = df , ax=axarr[0][1])\n",
      "sns.boxplot(y='Tenure',x = 'Exited', hue = 'Exited',data = df, ax=axarr[1][0])\n",
      "sns.boxplot(y='Balance',x = 'Exited', hue = 'Exited',data = df, ax=axarr[1][1])\n",
      "sns.boxplot(y='NumOfProducts',x = 'Exited', hue = 'Exited',data = df, ax=axarr[2][0])\n",
      "sns.boxplot(y='EstimatedSalary',x = 'Exited', hue = 'Exited',data = df, ax=axarr[2][1])\n",
      "plt.show()\n",
      "\n",
      "# Ingenieurie de la data\n",
      "df['BalanceSalaryRatio'] = df.Balance/df.EstimatedSalary\n",
      "df['TenureByAge'] = df.Tenure/(df.Age) # Tenure = mandat\n",
      "df['CreditScoreGivenAge'] = df.CreditScore/(df.Age)\n",
      "\n",
      "# Reordonne pour avoir les valeurs discretes à droite\n",
      "continuous_vars = ['CreditScore',  'Age', 'Tenure', 'Balance','NumOfProducts', 'EstimatedSalary', 'BalanceSalaryRatio',\n",
      "                   'TenureByAge','CreditScoreGivenAge']\n",
      "cat_vars = ['HasCrCard', 'IsActiveMember','Gender']\n",
      "df = df[['Exited'] + continuous_vars + cat_vars]\n",
      "print(df.head())\n",
      "\n",
      "df.loc[df.Gender == \"Male\", 'Gender'] = 1\n",
      "df.loc[df.Gender == \"Female\", 'Gender'] = 0\n",
      "print(df.head())\n",
      "\n",
      "# Equilibrage\n",
      "# Proportion de churn\n",
      "labels = 'Exited', 'Retained'\n",
      "sizes = [df.Exited[df.Exited==1].count(), df.Exited[df.Exited==0].count()]\n",
      "explode = (0, 0.1)\n",
      "fig1, ax1 = plt.subplots()\n",
      "ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n",
      "        shadow=True, startangle=90)\n",
      "ax1.axis('equal')\n",
      "plt.title(\"Proportion of customer churned and retained\", size = 20)\n",
      "plt.show()\n",
      "\n",
      "# Equilibrage\n",
      "nb = len(df[df.Exited==1])\n",
      "df = pd.concat([df[df.Exited==1], df[df.Exited==0][:nb]])\n",
      "\n",
      "df.to_csv(\"data/churn/churn_clean.csv\", index=False)\n",
      "\n",
      "Ce code effectue plusieurs actions sur un jeu de données contenant des informations sur des clients, notamment concernant leur attrition (churn). Voici les différentes étapes réalisées :\n",
      "\n",
      "1. Import des bibliothèques : pandas, sweetviz, matplotlib.pyplot et seaborn.\n",
      "\n",
      "2. Lecture du fichier CSV contenant les données des clients et affichage de sa forme (nombre de lignes et de colonnes).\n",
      "\n",
      "3. Anonymisation des données : suppression des colonnes \"RowNumber\", \"CustomerId\" et \"Surname\".\n",
      "\n",
      "4. Sélection des clients basés en France seulement en supprimant les autres valeurs de \"Geography\".\n",
      "\n",
      "5. Affichage des statistiques descriptives pour chaque variable du jeu de données.\n",
      "\n",
      "6. Commentaires et utilisation potentielle de la bibliothèque sweetviz pour générer un rapport d'analyse exploratoire des données. Cette partie est actuellement commentée.\n",
      "\n",
      "7. Affichage du nombre de valeurs uniques pour chaque variable du jeu de données.\n",
      "\n",
      "8. Analyse pragmatique des variables discrètes (catégorielles) : création de 2x2 sous-graphiques pour afficher des décomptes de chaque variable par rapport à la variable cible \"Exited\".\n",
      "\n",
      "9. Analyse pragmatique des variables continues : création de 3x2 sous-graphiques en utilisant des boîtes à moustaches pour afficher la distribution des variables continues par rapport à la variable cible \"Exited\".\n",
      "\n",
      "10. Ingénierie des données : création de nouvelles variables basées sur les variables existantes.\n",
      "\n",
      "11. Réorganisation des colonnes du jeu de données pour avoir les variables discrètes à droite.\n",
      "\n",
      "12. Conversion des valeurs de la variable \"Gender\" en 1 pour les hommes et 0 pour les femmes.\n",
      "\n",
      "13. Affichage des premières lignes du jeu de données après les modifications.\n",
      "\n",
      "14. Visualisation de la proportion de clients churnés et conservés à l'aide d'un graphique en camembert.\n",
      "\n",
      "15. Équilibrage des données en conservant le même nombre d'échantillons pour les clients churnés et les clients conservés.\n",
      "\n",
      "16. Sauvegarde du jeu de données nettoyé et équilibré au format CSV.\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/openai/churn_cleaning.py\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(text)\n",
    "\n",
    "client = OpenAI(api_key=key)\n",
    "\n",
    "completion = client.chat.completions.create(model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Expliques moi ce que fais ce code\"},\n",
    "        {\"role\": \"user\", \"content\": text}\n",
    "    ]\n",
    ")\n",
    "\n",
    "res = completion.choices[0].message\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19153,
     "status": "ok",
     "timestamp": 1704739335173,
     "user": {
      "displayName": "Cyril Vincent",
      "userId": "11684947983570556386"
     },
     "user_tz": -60
    },
    "id": "i41pK4iqYXHS",
    "outputId": "2845c34c-1a22-4fbe-bdc7-8322fe076ac6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai-whisper\n",
      "  Downloading openai-whisper-20231117.tar.gz (798 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.6/798.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.1.0)\n",
      "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.58.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (1.23.5)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.1.0+cu121)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (4.66.1)\n",
      "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (10.1.0)\n",
      "Collecting tiktoken (from openai-whisper)\n",
      "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper) (3.13.1)\n",
      "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper) (0.41.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2023.6.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (4.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (2023.6.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper) (1.3.0)\n",
      "Building wheels for collected packages: openai-whisper\n",
      "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=801356 sha256=b24b8cd3ed1b1bb824367d84ee69d45eea2e046cf87ab1bd8c428adea4d04237\n",
      "  Stored in directory: /root/.cache/pip/wheels/d0/85/e1/9361b4cbea7dd4b7f6702fa4c3afc94877952eeb2b62f45f56\n",
      "Successfully built openai-whisper\n",
      "Installing collected packages: tiktoken, openai-whisper\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed openai-whisper-20231117 tiktoken-0.5.2\n"
     ]
    }
   ],
   "source": [
    "!pip install openai-whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7PJ5OjKvYT4P"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "# import whisper # pip install openai-whisper\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u1yTR-cKYl8C"
   },
   "outputs": [],
   "source": [
    "class OpenAIService:\n",
    "\n",
    "    def __init__(self, whisper_model=\"base\", chat_model=\"gpt-3.5-turbo-1106\", chat_limit=16385):\n",
    "        # os.environ[\"path\"] += \";c:\\\\ffmpeg\\\\bin\"\n",
    "        # self.model = whisper.load_model(whisper_model)\n",
    "        with open(\"data/openai/openai.env\") as f:\n",
    "            key = f.read()\n",
    "        self.client = OpenAI(api_key=key)\n",
    "        self.chat_model = chat_model\n",
    "        self.chat_limit = chat_limit\n",
    "\n",
    "    def mp3_to_text(self, path):\n",
    "        result = self.model.transcribe(path)\n",
    "        return result[\"text\"].encode(\"utf-8\").decode()\n",
    "\n",
    "    def summary(self, text: str, nb: int=5) -> str:\n",
    "        completion = self.client.chat.completions.create(\n",
    "            model=self.chat_model,\n",
    "            messages=[{\"role\": \"system\", \"content\": f\"Fais moi un résumé en {nb} parties de ce texte\"},\n",
    "                {\"role\": \"user\", \"content\": text}])\n",
    "        return completion.choices[0].message.content\n",
    "\n",
    "    def correction(self, text: str, ponctuation=True, bank=True) -> str:\n",
    "        s = \"Corriges moi ce texte \"\n",
    "        if ponctuation:\n",
    "            s+=\"avec de la ponctuation \"\n",
    "        if bank:\n",
    "            s+=\"avec un vocabulaire bancaire\"\n",
    "        completion = self.client.chat.completions.create(\n",
    "            model=self.chat_model,\n",
    "            messages=[{\"role\": \"system\", \"content\": s},\n",
    "                      {\"role\": \"user\", \"content\": text}])\n",
    "        return completion.choices[0].message.content\n",
    "\n",
    "    def translate(self, text: str, langue=\"anglais\") -> str:\n",
    "        s = f\"Traduis moi ce texte en {langue}\"\n",
    "        completion = self.client.chat.completions.create(\n",
    "            model=self.chat_model,\n",
    "            messages=[{\"role\": \"system\", \"content\": s},\n",
    "                      {\"role\": \"user\", \"content\": text}])\n",
    "        return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401
    },
    "executionInfo": {
     "elapsed": 240,
     "status": "error",
     "timestamp": 1704988088548,
     "user": {
      "displayName": "Ludovic Fendorf",
      "userId": "15275327317811782890"
     },
     "user_tz": -60
    },
    "id": "icY_5CghYsp5",
    "outputId": "38c86ccf-cecd-4a93-d991-eb8bbf9a22de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI test\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/openai/openai.env'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-80bb86f46081>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"OpenAI test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mopenai\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpenAIService\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhisper_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"base\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtime0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# res = openai.mp3_to_text(\"data/openai/bank.mp3\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-01d9f3e40241>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, whisper_model, chat_model, chat_limit)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;31m# os.environ[\"path\"] += \";c:\\\\ffmpeg\\\\bin\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# self.model = whisper.load_model(whisper_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/openai/openai.env\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/openai/openai.env'"
     ]
    }
   ],
   "source": [
    "print(\"OpenAI test\")\n",
    "openai = OpenAIService(whisper_model=\"base\")\n",
    "time0 = datetime.datetime.now()\n",
    "\n",
    "# res = openai.mp3_to_text(\"data/openai/bank.mp3\")\n",
    "# print(res.strip())\n",
    "# with open(\"data/openai/bank.txt\", \"w\") as f:\n",
    "#     f.write(res)\n",
    "\n",
    "with open(\"data/openai/bank.txt\", \"r\", encoding=\"iso-8859-1\") as f:\n",
    "    res = f.read()\n",
    "correction = openai.correction(res, True, True)\n",
    "print(correction)\n",
    "with open(f\"data/openai/correction.txt\", \"w\") as f:\n",
    "    f.write(correction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UiXFvxzzNK1c"
   },
   "source": [
    "# Nouvelle section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8887,
     "status": "ok",
     "timestamp": 1704739724494,
     "user": {
      "displayName": "Cyril Vincent",
      "userId": "11684947983570556386"
     },
     "user_tz": -60
    },
    "id": "tLPmxW8dZ5eE",
    "outputId": "c9eb0423-5c30-4dd1-9c32-28f048296b7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Salimage, 32 ans, travaille comme conseiller clientèle à la Banque Postale, après avoir suivi une formation dispensée par La Poste et un centre de formation.\n",
      "\n",
      "2. Il a évolué de conseiller financier à conseiller clientèle, proposant les produits et services financiers de la Banque Postale et gérant un portefeuille de 1600 clients, avec une attention particulière pour les 500 clients principaux.\n",
      "\n",
      "3. Ses missions incluent la fourniture de conseils adaptés pour optimiser l'épargne et la constitution du patrimoine de ses clients, tout en développant le portefeuille à travers des actions sur les clients distanciers et de nouvelles approches client.\n",
      "\n",
      "4. Son quotidien est marqué par la préparation des entretiens, le suivi des dossiers, les entretiens avec les clients, la gestion des appels et du service après-vente, ainsi que la veille sur l'actualité économique et financière.\n",
      "\n",
      "5. Salimage trouve le contact avec la clientèle enrichissant et conseillerait à un jeune diplômé dans ce métier d'être persévérant, d'aimer les défis, d'être à l'écoute et de comprendre les autres, soulignant l'importance de la dimension conseil dans ce domaine.\n"
     ]
    }
   ],
   "source": [
    "summary = openai.summary(res)\n",
    "print(summary)\n",
    "with open(f\"data/openai/summary5.txt\", \"w\") as f:\n",
    "    f.write(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5669,
     "status": "ok",
     "timestamp": 1704739772612,
     "user": {
      "displayName": "Cyril Vincent",
      "userId": "11684947983570556386"
     },
     "user_tz": -60
    },
    "id": "hVQ99aQcaGfV",
    "outputId": "8511b09a-7ee0-4f7a-8032-5556f294ee4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Salimage, 32 years old, works as a customer advisor at La Banque Postale after completing training provided by La Poste and a training center.\n",
      "\n",
      "2. He progressed from being a financial advisor to a customer advisor, offering La Banque Postale's financial products and services and managing a portfolio of 1600 clients, with a particular focus on 500 key clients.\n",
      "\n",
      "3. His responsibilities include providing tailored advice to optimize his clients' savings and asset building, while also expanding the portfolio through actions targeting remote clients and new client approaches.\n",
      "\n",
      "4. His daily routine involves preparing for meetings, tracking files, meeting with clients, handling calls and after-sales service, as well as staying updated on economic and financial news.\n",
      "\n",
      "5. Salimage finds the client interaction enriching and would advise a young graduate in this profession to be persistent, enjoy challenges, be attentive and understanding, emphasizing the importance of advisory skills in this field.\n"
     ]
    }
   ],
   "source": [
    "summary_english = openai.translate(summary)\n",
    "print(summary_english)\n",
    "with open(f\"data/openai/summary_english.txt\", \"w\") as f:\n",
    "    f.write(summary_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13337,
     "status": "ok",
     "timestamp": 1704739814530,
     "user": {
      "displayName": "Cyril Vincent",
      "userId": "11684947983570556386"
     },
     "user_tz": -60
    },
    "id": "Ua66kRSbaKeh",
    "outputId": "d222f11e-46c3-45c3-a3fb-5f572779b969"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. サリマージュは32歳であり、郵便局でのトレーニングと研修センターでのトレーニングを受けた後、郵便局の顧客サービスアドバイザーとして働いています。\n",
      "\n",
      "2. 彼は金融アドバイザーから顧客サービスアドバイザーに昇進し、郵便局の金融商品やサービスを提供し、1600人の顧客ポートフォリオを管理し、特に500人の主要顧客に重点を置いています。\n",
      "\n",
      "3. 彼の任務には、顧客の貯蓄と資産の最適化のための適切なアドバイスの提供、遠隔地の顧客や新しいアプローチによるポートフォリオの開発が含まれています。\n",
      "\n",
      "4. 彼の日常は面談の準備、案件のフォローアップ、顧客との面談、電話対応やアフターサービスの管理、そして経済・金融の最新情報に対する情報収集で印象づけられています。\n",
      "\n",
      "5. サリマージュは顧客とのコンタクトを有益なものと感じ、この職業に就く若手の卒業生に、忍耐強く挑戦し、人々を聞き、理解すること、そしてこの分野でのアドバイスの重要性を強調しています。\n"
     ]
    }
   ],
   "source": [
    "summary_jpn = openai.translate(summary, \"japonais\")\n",
    "print(summary_jpn)\n",
    "with open(f\"data/openai/summary_jpn.txt\", \"wb\") as f:\n",
    "    f.write(summary_jpn.encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 460
    },
    "executionInfo": {
     "elapsed": 3468,
     "status": "error",
     "timestamp": 1704986154156,
     "user": {
      "displayName": "Cyril Vincent",
      "userId": "11684947983570556386"
     },
     "user_tz": -60
    },
    "id": "IAPMIhSLXvxo",
    "outputId": "d7c70b09-8b05-4c6a-f875-705f4a469434"
   },
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'code': 'rate_limit_exceeded', 'message': 'Rate limit exceeded for images per minute in organization org-HsnxNdW6VmuOpbzwkU3gojNp. Limit: 0/1min. Current: 1/1min. Please visit https://platform.openai.com/docs/guides/rate-limits to learn how to increase your rate limit.', 'param': None, 'type': 'requests'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-b5656f89fe9f>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m response = client.images.generate(\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"dall-e-3\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"La banque et Sopra Steria en 2030\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/resources/images.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, prompt, model, n, quality, response_format, size, style, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    247\u001b[0m           \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOverride\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlevel\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \"\"\"\n\u001b[0;32m--> 249\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    250\u001b[0m             \u001b[0;34m\"/images/generations\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m             body=maybe_transform(\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1110\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m         )\n\u001b[0;32m-> 1112\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m     def patch(\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mstream_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_StreamT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m     ) -> ResponseT | _StreamT:\n\u001b[0;32m--> 859\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    860\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    932\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mretries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m    935\u001b[0m                     \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m    980\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 982\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    983\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    932\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mretries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m    935\u001b[0m                     \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m    980\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 982\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    983\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 949\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m         return self._process_response(\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'code': 'rate_limit_exceeded', 'message': 'Rate limit exceeded for images per minute in organization org-HsnxNdW6VmuOpbzwkU3gojNp. Limit: 0/1min. Current: 1/1min. Please visit https://platform.openai.com/docs/guides/rate-limits to learn how to increase your rate limit.', 'param': None, 'type': 'requests'}}"
     ]
    }
   ],
   "source": [
    "client = OpenAI(api_key=key)\n",
    "\n",
    "response = client.images.generate(\n",
    "  model=\"dall-e-3\",\n",
    "  prompt=\"La banque et Sopra Steria en 2030\",\n",
    "  size=\"1024x1024\",\n",
    "  quality=\"standard\",\n",
    "  n=1,\n",
    ")\n",
    "\n",
    "image_url = response.data[0].url\n",
    "print(image_url)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
